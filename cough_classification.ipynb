{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Research references:\n",
    "#1) Dry/wet cough classification: https://link.springer.com/article/10.1007/s10439-013-0741-6\n",
    "#2) Pneumonia classification: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.io.wavfile import read\n",
    "#import wave\n",
    "import numpy as np\n",
    "import os\n",
    "import sox\n",
    "#import pywt #wavelets\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.utils import mediainfo\n",
    "import matplotlib.pyplot as plt\n",
    "import python_speech_features as spe_feats\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from scipy.signal import lfilter\n",
    "import librosa\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply pre-emphasis (high-pass) filter\n",
    "def apply_preEmph(x):\n",
    "    x_filt = lfilter([1., -0.63], 1, x)\n",
    "    return x_filt\n",
    "        \n",
    "#Obtain autocorrelation\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[int((result.size+1)/2):] #Note: other people use re.size/2:, but this does not work for me \n",
    "                                   # TODO: check consistency in other computers\n",
    "#Get fundamental frequency (F0)\n",
    "def get_F0(x,fs):\n",
    "    #autocorrelation-based method to extract F0\n",
    "    xcorr_arr = autocorr(x)\n",
    "    \n",
    "    #looking for F0 in the frequency interval 50-500Hz, but we search in time domain\n",
    "    min_ms = round(fs/500)\n",
    "    max_ms = round(fs/50)\n",
    "    \n",
    "    xcorr_slot = xcorr_arr[max_ms+1:2*max_ms+1]\n",
    "    xcorr_slot = xcorr_slot[min_ms:max_ms]\n",
    "    t0 = np.argmax(xcorr_slot)\n",
    "    F0 = fs/(min_ms+t0-1)\n",
    "    return F0\n",
    "\n",
    "#Get formants\n",
    "def get_formants(x, lp_order):\n",
    "    #LP-based method\n",
    "    \n",
    "    #compute lp coefficients\n",
    "    a = librosa.lpc(x, lp_ord)\n",
    "\n",
    "    #get roots from lp coefficients\n",
    "    rts = np.roots(a)\n",
    "    rts = [r for r in rts if np.imag(r) >= 0]\n",
    "\n",
    "    #get angles\n",
    "    angz = np.arctan2(np.imag(rts), np.real(rts))\n",
    "\n",
    "    #get formant frequencies\n",
    "    formants = sorted(angz * (fs_targ / (2 * math.pi)))\n",
    "    \n",
    "    return formants\n",
    "\n",
    "#Extract frequencies\n",
    "def feature_extraction(x,fs,feats_df,lp_ord,ID,label):\n",
    "#Extract features from signal x (identified as ID), and concatenate them to dataframe feats_df\n",
    "#Features' reference: (see Appendix)\n",
    "#[1]https://link.springer.com/article/10.1007/s10439-013-0741-6\n",
    "#[2]https://espace.library.uq.edu.au/data/UQ_344963/s41943203_phd_submission.pdf?dsi_version=c5434db897ab74b192ca295a9eeca041&Expires=1585086202&Key-Pair-Id=APKAJKNBJ4MJBJNC6NLQ&Signature=c8k8DmG~KIxg0ToTO8rebm2MzHneCzJGkjSFRB7BYTEQ-MHXEr0ocHmISrldP3hFf9qmeiL11ezyefcNeRVeKIQ9PVjOl9pn7rXWcjA1o2voPn1VnDd8n7G2cT31apdj0LNMclhlXRPnCsGD66qDRqa3d-xaqqXhEqU73aw3ZgBgroO213MfJOqFhJxxXo2QEia0bSlDRTeX9KhSczFK-IFTPC6GwFL2L04por8pQRI3HF7E3f26O9zp9OhkwxSU9qfJah20WxZLA4PxREdv7JGoVBinR6T0mTcIaQi~B4IzYjSPSsTTADMNk5znVYIvSqgtMT~DY~qwlfq4SRdFjQ__\n",
    "  \n",
    "    #0)Wavelets #TODO\n",
    "    \n",
    "    #HAMMING WINDOW USED IN MFCC TODO: CHECK IF USING THAT ONE\n",
    "    #DOUBT: if log-energy feature is included, should I also include the first mfcc coefficient (c0) ?\n",
    "    #1)mfcc\n",
    "    mfcc_feat = spe_feats.mfcc(x,fs)\n",
    "    \n",
    "    #TODO:consider also adding deltas and deltadeltas from the mfcc feat to capture temporal variations\n",
    "          \n",
    "    #2)zero-crossing rate\n",
    "    zcr_feat = (((x[:-1] * x[1:]) < 0).sum())/len(x)\n",
    "    \n",
    "    #3)Formant frequencies\n",
    "    #using LP-coeffcs-based method\n",
    "    form= get_formants(x, lp_ord)\n",
    "    \n",
    "    #we keep just the first 4 formants\n",
    "    formant_feats = form[0:4]\n",
    "    \n",
    "    #4)Log-energy\n",
    "    logEnergy_feat = np.log10( ( (np.power(x,2)).sum()/len(x) ) + eps)    \n",
    "    \n",
    "    #5)Pitch (F0)\n",
    "    F0_feat = get_F0(x,fs)\n",
    "    \n",
    "    #6)Kurtosis\n",
    "    kurt_feat = kurtosis(x)\n",
    "    \n",
    "    #7)Bispectrum Score (BGS)\n",
    "    #TODO\n",
    "    \n",
    "    #8)Non-Gaussianity Score (NGS)\n",
    "    #TODO\n",
    "    # Do kernel density estimation\n",
    "    #p = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(seg_iarray)\n",
    "    #q generate Gaussian distribution? using mean and standard dev of reference data, plus min and max values?\n",
    "    \n",
    "    #9) Adding skewness as measure of non-gaussianity (not in paper)\n",
    "    skew_feat = skew(x)\n",
    "    \n",
    "    #DOUBT: 10) Shannon entropy GETTING -inf in all cases, WHY??? Don't include until fixed\n",
    "    #entropy_feat = entropy(x)\n",
    "    #Maybe compute directly to check\n",
    "\n",
    "    feats_df = feats_df.append(pd.DataFrame({'Id': ID, 'mfcc': [mfcc_feat], \n",
    "                                       'kurtosis': kurt_feat, 'logEnergy': logEnergy_feat, 'zcr': zcr_feat,\n",
    "                                       'formants': [formant_feats], 'F0': F0_feat, 'skew': skew_feat,\n",
    "                                             'label': label}, index=[0]), ignore_index=True, sort=False)\n",
    "    return feats_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize data frame of features:\n",
    "\n",
    "feats = pd.DataFrame([])\n",
    "\n",
    "#tiny constant value\n",
    "eps = sys.float_info.epsilon\n",
    "\n",
    "#Features' settings:\n",
    "\n",
    "fs_targ = 16000 # set all audios to this sampling frequency\n",
    "n_channels_targ = 1\n",
    "\n",
    "#framing\n",
    "winlen=0.025\n",
    "winstep=0.01\n",
    "\n",
    "#mfcc\n",
    "mfcc_coeffcs= 12 #as paper (https://link.springer.com/article/10.1007/s10439-013-0741-6)\n",
    "\n",
    "lp_ord = int(round(2 + fs_targ/1000)) #standard rule of thumb for LP oder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_file: data/YT_set/wavs/norm/Another Girl Coughing-iYxUHA-Pwsk-Dry_NORM.wav already exists and will be overwritten on build\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/YT_set/wavs/norm/Another Girl Coughing-iYxUHA-Pwsk-Dry_NORM.wav\n",
      "Another Girl Coughing-iYxUHA-Pwsk-Dry.wav\n",
      "Pwsk\n",
      "Dry\n",
      "High-pass filtering...\n",
      "Computing features...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "norm_skip = False #skip normalization step (because it has been done previously)\n",
    "\n",
    "#s = read(audiofile)\n",
    "FOLDER_PATH = 'data/YT_set/wavs/1/'\n",
    "for file_name in os.listdir(FOLDER_PATH):\n",
    "    \n",
    "    fname_noExt = os.path.splitext(file_name)[0] #file name without extension\n",
    "    \n",
    "    #full path file name\n",
    "    full_fname = FOLDER_PATH+file_name\n",
    "    #print(full_fname)\n",
    "    \n",
    "    #TODO: put normalized wavs in other folder\n",
    "    #name for normalization\n",
    "    NORM_FOLDER_PATH = 'data/YT_set/wavs/norm/'\n",
    "    norm_fname = NORM_FOLDER_PATH + os.path.splitext(file_name)[0] + '_NORM.wav'\n",
    "    \n",
    "    if norm_skip is False: \n",
    "        ## Normalization\n",
    "        \n",
    "        #level to same dB\n",
    "        tfm = sox.Transformer()\n",
    "        tfm.gain(gain_db=0.0, normalize=False, limiter=False, balance=None)\n",
    "        #downsample to 16kHz and 1 channel\n",
    "        tfm.convert(samplerate=fs_targ, n_channels=n_channels_targ, bitdepth=None) \n",
    "        #tfm.norm(db_level=0.0)\n",
    "    \n",
    "        # create the output normalized audio\n",
    "        \n",
    "        print(norm_fname)\n",
    "        tfm.build(full_fname, norm_fname)\n",
    "        tfm.effects_log\n",
    "    \n",
    "    # load normalized audio\n",
    "    s = AudioSegment.from_wav(norm_fname)\n",
    "    #sampling rate:\n",
    "    info = mediainfo(norm_fname)\n",
    "    fs = float(info['sample_rate'])\n",
    "    \n",
    "    #get ID of recording\n",
    "    ID = fname_noExt.split('-')[-2] #for the current type of naming\n",
    "    print(file_name)\n",
    "    print(ID)\n",
    "    \n",
    "    #get label\n",
    "    label = fname_noExt.split('-')[-1] #for the current type of naming\n",
    "    print(label)\n",
    "    \n",
    "    ## Segmentation of cough streams (silence-based)\n",
    "    #min_silence_len in ms, silence_thresh in dB\n",
    "    s_segments = split_on_silence (s, min_silence_len = 600, silence_thresh = -30)\n",
    "    ## TODO: set more accurate thresholds, or find other way to split (variance-based?)\n",
    "    \n",
    "    #convert s_segments to numpy array format\n",
    "    AudioSegment2numpy_arr = lambda x: np.asarray(x.get_array_of_samples())\n",
    "    s_segments_np = list(map(AudioSegment2numpy_arr, s_segments))\n",
    "    \n",
    "    print('High-pass filtering...')\n",
    "    #pre-emphasis filtering to each segment\n",
    "    preEmph_filtering = lambda x: apply_preEmph(x)\n",
    "    s_segments_filt = list(map(preEmph_filtering, s_segments_np))\n",
    "    \n",
    "    #TODO\n",
    "    #2) the segment is divided into X non-overlapping subsegments (X=3 for dry/wet cough paper,\n",
    "    #X=12 for pneumonia paper)\n",
    "    #TODO: window framing: I think maybe the segments need to be windowed with non-overlapping frames? \n",
    "    #(see frame settings at beggining of notebook)\n",
    "    \n",
    "    print('Computing features...')\n",
    "    #Feature extraction for each segment\n",
    "    \n",
    "    #(lambda function doesn't work )\n",
    "    #feat_extr_step = lambda x, fs, feats_df, lp_ord, ID: feature_extraction(x,fs,feats_df,lp_ord,ID)\n",
    "    #feats = feat_extr_step(s_segments_filt,fs,feats,lp_ord,ID)\n",
    "    for idx, seg_i in enumerate(s_segments_filt):\n",
    "        print(idx)\n",
    "        feats = feature_extraction(seg_i,fs,feats,lp_ord,ID,label)\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>logEnergy</th>\n",
       "      <th>zcr</th>\n",
       "      <th>formants</th>\n",
       "      <th>F0</th>\n",
       "      <th>skew</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pwsk</td>\n",
       "      <td>[[-36.04365338911715, 0.0, -3.2076317224819155...</td>\n",
       "      <td>10.261335</td>\n",
       "      <td>7.033567</td>\n",
       "      <td>0.266776</td>\n",
       "      <td>[615.4712130184763, 1009.0686183582355, 2031.2...</td>\n",
       "      <td>213.333333</td>\n",
       "      <td>-0.035899</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pwsk</td>\n",
       "      <td>[[9.60457382248319, -30.766157147256642, -13.5...</td>\n",
       "      <td>7.269807</td>\n",
       "      <td>7.445741</td>\n",
       "      <td>0.352220</td>\n",
       "      <td>[795.7480969526919, 900.8716953508034, 2145.66...</td>\n",
       "      <td>152.380952</td>\n",
       "      <td>-0.003134</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pwsk</td>\n",
       "      <td>[[9.681963295223953, -29.406913382520816, -7.9...</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>6.289370</td>\n",
       "      <td>0.097091</td>\n",
       "      <td>[551.2383040575314, 1045.736114981098, 1655.39...</td>\n",
       "      <td>163.265306</td>\n",
       "      <td>-0.180193</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pwsk</td>\n",
       "      <td>[[9.888757595969555, -25.51711795819935, 0.038...</td>\n",
       "      <td>5.542743</td>\n",
       "      <td>7.493706</td>\n",
       "      <td>0.326300</td>\n",
       "      <td>[715.756479741867, 1567.4161317150094, 1734.37...</td>\n",
       "      <td>326.530612</td>\n",
       "      <td>-0.019812</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pwsk</td>\n",
       "      <td>[[10.639917255049568, -25.79301496979985, -2.0...</td>\n",
       "      <td>10.933372</td>\n",
       "      <td>7.087280</td>\n",
       "      <td>0.259046</td>\n",
       "      <td>[558.7012952572038, 1242.9710489844333, 2103.9...</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>-0.132976</td>\n",
       "      <td>Dry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                               mfcc   kurtosis  \\\n",
       "0  Pwsk  [[-36.04365338911715, 0.0, -3.2076317224819155...  10.261335   \n",
       "1  Pwsk  [[9.60457382248319, -30.766157147256642, -13.5...   7.269807   \n",
       "2  Pwsk  [[9.681963295223953, -29.406913382520816, -7.9...   0.573200   \n",
       "3  Pwsk  [[9.888757595969555, -25.51711795819935, 0.038...   5.542743   \n",
       "4  Pwsk  [[10.639917255049568, -25.79301496979985, -2.0...  10.933372   \n",
       "\n",
       "   logEnergy       zcr                                           formants  \\\n",
       "0   7.033567  0.266776  [615.4712130184763, 1009.0686183582355, 2031.2...   \n",
       "1   7.445741  0.352220  [795.7480969526919, 900.8716953508034, 2145.66...   \n",
       "2   6.289370  0.097091  [551.2383040575314, 1045.736114981098, 1655.39...   \n",
       "3   7.493706  0.326300  [715.756479741867, 1567.4161317150094, 1734.37...   \n",
       "4   7.087280  0.259046  [558.7012952572038, 1242.9710489844333, 2103.9...   \n",
       "\n",
       "           F0      skew label  \n",
       "0  213.333333 -0.035899   Dry  \n",
       "1  152.380952 -0.003134   Dry  \n",
       "2  163.265306 -0.180193   Dry  \n",
       "3  326.530612 -0.019812   Dry  \n",
       "4  320.000000 -0.132976   Dry  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pre-proces the dataframe prior to use with a model (normalize if needed for the model, check there is no NaNs...)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cough sound\n",
    "#Breathing rate\n",
    "#Breathing rhytm (consistence smoothness)\n",
    "#Cough rate\n",
    "#Panic level\n",
    "#Hoarseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
